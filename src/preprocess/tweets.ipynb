{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"tweets.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[],"mount_file_id":"1JaZ0hN_KWkcsdUjbqk8yRjLAgHswWTDS","authorship_tag":"ABX9TyOoY+P+n5fcX0a4jUg5+Uh3"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"QyeQ4rHTKDkI"},"source":["# Tweets preprocessing script\n","\n","## Configuration"]},{"cell_type":"code","metadata":{"id":"-WoNkjFyJtsv"},"source":["# Configure modules path\n","SRC_DIRECTORY = \"/content/drive/My Drive/project/src\"\n","\n","# Configure script\n","DATASET_RAW_TWEETS_DIRECTORY = \"/content/drive/My Drive/project/dataset/raw/tweets\"\n","DATASET_PREPROCESSED_TWEETS_DIRECTORY = \"/content/drive/My Drive/project/dataset/preprocessed/tweets\"\n","PREPROCESS_CONFIG = {\n","  \"raw\": DATASET_RAW_TWEETS_DIRECTORY + \"/dataset.csv\",\n","  \"save_eda\": DATASET_PREPROCESSED_TWEETS_DIRECTORY + \"/dataset_eda.csv\",\n","  \"save_eda_pics\": DATASET_PREPROCESSED_TWEETS_DIRECTORY + \"/dataset_eda_pics.csv\",\n","  \"save_model\": DATASET_PREPROCESSED_TWEETS_DIRECTORY + \"/dataset_model.csv\"\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FNbQG8Q-NaG_"},"source":["## Import required modules"]},{"cell_type":"code","metadata":{"id":"q0UGP9OnMF9E"},"source":["import sys\n","sys.path.insert(0, SRC_DIRECTORY)\n","\n","import numpy as np\n","import pandas as pd\n","import utils.datetime"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0nrN63QoNce2"},"source":["## Preprocessing functions\n","### Util functions"]},{"cell_type":"code","metadata":{"id":"KxWEIrA0dYsv"},"source":["def read_tweets_file(file_path):\n","  df = pd.read_csv(file_path)\n","  df['to'].fillna('', inplace=True)\n","  df['geo'].fillna('', inplace=True)\n","  df['mentions'].fillna('', inplace=True)\n","  df['hashtags'].fillna('', inplace=True)\n","  return df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kyITYAZAd1hf"},"source":["### Preprocessing function for Explorative Data Analysis"]},{"cell_type":"markdown","metadata":{"id":"ahD0jrXGeaYX"},"source":["Expected columns in DataFrame:\n","\n","[(0, 'date'), (1, 'username'), (2, 'to'), (3, 'replies'), (4, 'retweets'), (5, 'favorites'), (6, 'text'), (7, 'geo'), (8, 'mentions'), (9, 'hashtags'), (10, 'id'), (11, 'permalink')]"]},{"cell_type":"code","metadata":{"id":"76IrQmXBsPwB"},"source":["import re\n","def filter_tweet_links_mentions_hashtags(tweet_text):\n","  return re.sub(\n","    r\"@\\S+|https?://\\S+|pic.twitter.com\\S+|…|#\\S+\", \"\", tweet_text\n","  ).strip()\n","\n","def filter_tweet_links(tweet_text):\n","  return re.sub(\n","    r\"https?://\\S+|pic.twitter.com\\S+|…|\", \"\", tweet_text\n","  ).strip()\n","\n","def remove_bad_tokens(tweet_text):\n","  return tweet_text.replace(\"pic\", \"\").replace(\"http\", \"\").replace(\"https\", \"\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g-CUwl85NlsW"},"source":["def preprocess_raw_tweets_eda(tweets_df):\n","  tweets = tweets_df.values.copy()\n","\n","  filtered_tweets = []\n","\n","  for i in range(tweets.shape[0]):\n","    \n","    dt = utils.datetime.parse_datetime_str(\n","      tweets[i, 0], \"%d-%m-%y %H:%M\", \"+0000\"\n","    )\n","    dt = utils.datetime.convert_datetime_timezone(dt, \"America/New_York\")\n","\n","    tweet_text_filtered = filter_tweet_links_mentions_hashtags(tweets[i, 6])\n","\n","    if len(tweet_text_filtered) > 0:\n","      filtered_tweets.append([\n","        # Username\n","        tweets[i, 1],\n","        # Date and time\n","        dt.year,\n","        dt.month,\n","        dt.day,\n","        dt.hour,\n","        dt.minute,\n","        # Text\n","        tweet_text_filtered,\n","        # Reply to (empty string if it's original tweet)\n","        tweets[i, 2],\n","        # Replies\n","        tweets[i, 3],\n","        # Retweets\n","        tweets[i, 4],\n","        # Favorites\n","        tweets[i, 5],\n","        # Mentions\n","        remove_bad_tokens(tweets[i, 8]),\n","        # Hashtags\n","        remove_bad_tokens(tweets[i, 9])\n","      ])\n","\n","  return pd.DataFrame(np.asarray(filtered_tweets), columns = [\n","    'username', 'year', 'month', 'day', 'hour', 'minute', 'text',\n","    'reply_to', 'replies', 'retweets', 'favorites',\n","    'mentions', 'hashtags'\n","  ])\n","\n","def extract_pics(tweets_df):\n","  tweets = tweets_df.values.copy()\n","\n","  pics = []\n","\n","  for i in range(tweets.shape[0]):\n","    if \"pic.twitter.com\" in tweets[i, 6]:\n","      pics.append([\n","        # Username\n","        tweets[i, 1],\n","        # Text\n","        tweets[i, 6],\n","        # Favorites\n","        tweets[i, 5]  \n","      ])\n","  \n","  return pd.DataFrame(np.asarray(pics), columns = [\n","    \"username\",\n","    \"text\",\n","    \"favorites\"\n","  ])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HdGesoz8YHT6"},"source":["### Preprocessing data for model"]},{"cell_type":"code","metadata":{"id":"1-bZLK4GxsXj"},"source":["def preprocess_raw_tweets_model(tweets_df):\n","  tweets = tweets_df.values.copy()\n","  print(tweets.shape)\n","  \n","  tweets = tweets[tweets[:, 2] == \"\"]\n","  print(tweets.shape)\n","\n","  filtered_tweets = []\n","\n","  for i in range(tweets.shape[0]):\n","    \n","    dt = utils.datetime.parse_datetime_str(\n","      tweets[i, 0], \"%d-%m-%y %H:%M\", \"+0000\"\n","    )\n","    timestamp = utils.datetime.datetime_to_timestamp(dt)\n","\n","    tweet_text_filtered = filter_tweet_links(tweets[i, 6])\n","\n","    if len(tweet_text_filtered) > 0:\n","      filtered_tweets.append([\n","        # Username\n","        tweets[i, 1],\n","        # Timestamp\n","        timestamp,\n","        # Text\n","        tweet_text_filtered,\n","        # Likes\n","        tweets[i, 5]\n","      ])\n","\n","  return pd.DataFrame(np.asarray(filtered_tweets), columns = [\n","    \"username\",\n","    \"timestamp\",\n","    \"text\",\n","    \"likes\"\n","  ])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m_J88dAqy6xo"},"source":["## Preprocess the data"]},{"cell_type":"markdown","metadata":{"id":"i9JuCyEozZQd"},"source":["### Explorative Data Analysis"]},{"cell_type":"code","metadata":{"id":"pnCbrhTMzVrp"},"source":["print(PREPROCESS_CONFIG['raw'])\n","\n","tweets_df = read_tweets_file(PREPROCESS_CONFIG['raw'])\n","tweets = preprocess_raw_tweets_eda(tweets_df)\n","\n","print(\"tweets\")\n","print(tweets.describe())\n","\n","tweets.to_csv(PREPROCESS_CONFIG[\"save_eda\"])\n","\n","if \"save_eda_pics\" in PREPROCESS_CONFIG:\n","  pics = extract_pics(tweets_df)\n","  pics.to_csv(PREPROCESS_CONFIG[\"save_eda_pics\"])\n","\n","  print(\"pics\")\n","  print(pics.describe())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z0kW4OOZzWhQ"},"source":["### Model"]},{"cell_type":"code","metadata":{"id":"wrIjFY3LyxNz"},"source":["print(PREPROCESS_CONFIG['raw'])\n","\n","tweets_df = read_tweets_file(PREPROCESS_CONFIG['raw'])\n","tweets = preprocess_raw_tweets_model(tweets_df)\n","\n","print(\"tweets\")\n","print(tweets.describe())\n","\n","tweets.to_csv(PREPROCESS_CONFIG[\"save_model\"], index=False)\n","\n","print()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NcH9ZUee91rh"},"source":[""],"execution_count":null,"outputs":[]}]}